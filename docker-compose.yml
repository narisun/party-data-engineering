# docker-compose.yml

services:
  # 1. Orchestration: Airflow
  postgres_airflow:
    image: postgres:16
    container_name: postgres_airflow
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - airflow_db_data:/var/lib/postgresql/data
    networks:
      - de_net

  airflow-init:
    image: apache/airflow:2.9.2-python3.11
    container_name: airflow_init
    user: "${AIRFLOW_UID:-50000}:0"
    depends_on:
      - postgres_airflow
    env_file:
      - .env
    environment:
      - _PIP_ADDITIONAL_REQUIREMENTS=apache-airflow-providers-apache-spark==4.1.2
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres_airflow:5432/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    volumes:
      - ./:/opt/airflow
    command: >
      bash -c "airflow db init && airflow users create --username admin --firstname Airflow --lastname Admin --role Admin --email admin@example.com --password admin"
    networks:
      - de_net

  airflow-webserver:
    image: apache/airflow:2.9.2-python3.11
    container_name: airflow_webserver
    user: "${AIRFLOW_UID:-50000}:0"
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    env_file:
      - .env
    environment:
      - _PIP_ADDITIONAL_REQUIREMENTS=apache-airflow-providers-apache-spark==4.1.2
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres_airflow:5432/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./jobs:/opt/airflow/jobs
      - ./gx:/opt/airflow/gx
    ports:
      - "8080:8080"
    command: webserver
    networks:
      - de_net

  airflow-scheduler:
    image: apache/airflow:2.9.2-python3.11
    container_name: airflow_scheduler
    user: "${AIRFLOW_UID:-50000}:0"
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    env_file:
      - .env
    environment:
      - _PIP_ADDITIONAL_REQUIREMENTS=apache-airflow-providers-apache-spark==4.1.2
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres_airflow:5432/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./jobs:/opt/airflow/jobs
      - ./gx:/opt/airflow/gx
    command: scheduler
    networks:
      - de_net

  # 2. Distributed Storage: MinIO (S3-compatible)
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    env_file:
      - .env
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      - de_net

  # 3. Distributed Compute: Spark
  spark-master:
    image: bitnami/spark:3.5.1
    container_name: spark-master
    command: >
      /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8081:8080"
      - "7077:7077"
    # --- FIX: Mount volumes so Spark can access the files ---
    volumes:
      - ./data:/opt/airflow/data
      - ./jobs:/opt/airflow/jobs
      - ./gx:/opt/airflow/gx
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    networks:
      - de_net

  spark-worker:
    image: bitnami/spark:3.5.1
    container_name: spark-worker
    depends_on:
      - spark-master
    env_file:
      - .env
    command: >
      /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    # --- FIX: Mount volumes so Spark can access the files ---
    volumes:
      - ./data:/opt/airflow/data
      - ./jobs:/opt/airflow/jobs
      - ./gx:/opt/airflow/gx
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    networks:
      - de_net

  # 4. Query Engine: Trino
  trino-coordinator:
    image: trinodb/trino:448
    container_name: trino
    ports:
      - "8082:8080"
    volumes:
      - ./trino/catalog:/etc/trino/catalog
    networks:
      - de_net

  # 5. Visualization: Apache Superset
  superset:
    image: apache/superset:4.0.1
    container_name: superset
    depends_on:
      - trino-coordinator
    ports:
      - "8088:8080"
    env_file:
      - .env
    command: >
      bash -c "pip install trino sqlalchemy-trino &&
               superset db upgrade &&
               superset fab create-admin --username admin --firstname Superset --lastname Admin --email admin@superset.com --password admin &&
               superset init &&
               superset run -p 8088 --with-threads --reload --debugger"
    networks:
      - de_net

volumes:
  airflow_db_data:
  minio_data:

networks:
  de_net:
    driver: bridge