
x-airflow-image: &airflow_image my-airflow-spark:2.8.2

services:
  postgres_airflow:
    image: postgres:16
    container_name: postgres_airflow
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - airflow_db_data:/var/lib/postgresql/data
    networks: [de_net]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 10s
      timeout: 5s
      retries: 10

  airflow-init:
    env_file: [.env]
    image: *airflow_image
    container_name: airflow_init
    depends_on:
      postgres_airflow:
        condition: service_healthy
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres_airflow:5432/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW__WEBSERVER__SECRET_KEY:-your_airflow_secret_key_change_me}
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      #- ./dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./jobs:/opt/airflow/jobs
      - ./gx:/opt/airflow/gx
    command: >
      bash -lc '
      set -euo pipefail
      echo ">>> DB migrate"
      airflow db migrate || airflow db upgrade
      echo ">>> Ensure admin"
      airflow users create \
        --role Admin --username admin --password admin \
        --firstname Admin --lastname User --email admin@example.com || true
      echo ">>> Ensure spark_default"
      airflow connections delete spark_default || true
      airflow connections add spark_default \
        --conn-type spark \
        --conn-extra "{\"master\":\"spark://spark-master:7077\",\"deploy_mode\":\"client\",\"spark_binary\":\"/opt/spark/bin/spark-submit\"}"
      echo ">>> Done"
      '
    networks: [de_net]

  airflow-webserver:
    env_file: [.env]
    build:
      context: ./docker/airflow
    image: *airflow_image
    container_name: airflow_webserver
    depends_on:
      postgres_airflow:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres_airflow:5432/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__WEBSERVER__WORKERS=2
      - AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT=120
      - AIRFLOW__WEBSERVER__WEB_SERVER_MASTER_TIMEOUT=300
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      # Fallback connection (encoded as URI)
      # - AIRFLOW_CONN_SPARK_DEFAULT=spark://?extra__spark__master=spark://spark-master:7077&extra__spark__deploy_mode=client&extra__spark__spark_binary=/opt/spark/bin/spark-submit
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./jobs:/opt/airflow/jobs
      - ./gx:/opt/airflow/gx
    ports:
      - "8080:8080"
    command: webserver
    networks: [de_net]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    env_file: [.env]
    image: *airflow_image
    container_name: airflow_scheduler
    depends_on:
      postgres_airflow:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres_airflow:5432/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      # Fallback connection (encoded as URI)
      # - AIRFLOW_CONN_SPARK_DEFAULT=spark://?extra__spark__master=spark://spark-master:7077&extra__spark__deploy_mode=client&extra__spark__spark_binary=/opt/spark/bin/spark-submit
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./jobs:/opt/airflow/jobs
      - ./gx:/opt/airflow/gx
    command: scheduler
    networks: [de_net]

  minio:
    image: minio/minio:latest
    container_name: minio
    ports: ["9000:9000", "9001:9001"]
    env_file: [.env]
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks: [de_net]

  spark-master:
    image: bitnami/spark:3.5.1
    container_name: spark-master
    command: /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports: ["8081:8080", "7077:7077"]
    volumes:
      - ./data:/opt/airflow/data
      - ./jobs:/opt/airflow/jobs
      - ./gx:/opt/airflow/gx
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    networks: [de_net]

  spark-worker:
    image: bitnami/spark:3.5.1
    container_name: spark-worker
    depends_on: [spark-master]
    command: /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    volumes:
      - ./data:/opt/airflow/data
      - ./jobs:/opt/airflow/jobs
      - ./gx:/opt/airflow/gx
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    networks: [de_net]

  trino-coordinator:
    image: trinodb/trino:448
    container_name: trino
    ports: ["8082:8080"]
    volumes:
      - ./trino/catalog:/etc/trino/catalog
    networks: [de_net]

  superset:
    image: apache/superset:4.0.1
    container_name: superset
    depends_on: [trino-coordinator]
    ports: ["8088:8080"]
    env_file: [.env]
    command: >
      bash -c "pip install trino sqlalchemy-trino &&
               superset db upgrade &&
               superset fab create-admin --username admin --firstname Superset --lastname Admin --email admin@superset.com --password admin &&
               superset init &&
               superset run -p 8088 --with-threads --reload --debugger"
    networks: [de_net]

volumes:
  airflow_db_data:
  minio_data:

networks:
  de_net:
    driver: bridge
